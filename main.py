import logging
from typing import Dict, Any, List

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def run_assessment_analysis(file_path: str = None, text_content: str = None, uoc_data: Dict[str, Any] = None, api_key: str = "", mapping_only: bool = False, questions: List[Dict[str, Any]] = None):
    """
    Properly run the assessment analysis with error handling for Assessment Validator
    
    Args:
        file_path (str): Path to assessment file (optional if text_content provided)
        text_content (str): Raw text content (optional if file_path provided)
        uoc_data (Dict): UoC data structure
        api_key (str): API key for LLM services
        mapping_only (bool): If True, only perform mapping (skip extraction)
        questions (List): Pre-extracted questions (used when mapping_only=True)
        
    Returns:
        Dict: Complete analysis results or error information
    """
    try:
        logger.info("🚀 Starting assessment analysis...")
        
        # Validate inputs
        if not mapping_only and not file_path and not text_content:
            raise ValueError("Either file_path or text_content must be provided")
        
        if not uoc_data:
            raise ValueError("UoC data must be provided")
        
        if mapping_only and not questions:
            raise ValueError("Questions must be provided when mapping_only=True")
        
        # Step 1: Extract questions (if not mapping_only)
        if not mapping_only:
            logger.info("📄 Step 1: Extracting questions...")
            from agents.extract_agent import ExtractAgent
            
            extract_agent = ExtractAgent(api_key=api_key)
            
            if file_path:
                questions = extract_agent.execute(file_path)
            else:
                questions = extract_agent.execute_from_text(text_content)
            
            if not questions:
                logger.warning("⚠️  No questions extracted!")
                return {
                    'success': False,
                    'error': 'No questions could be extracted from the document',
                    'questions': [],
                    'mappings': {'mappings': [], 'analysis': {}}
                }
            
            logger.info(f"✅ Extracted {len(questions)} questions")
            
            # Log question details for debugging
            for i, q in enumerate(questions[:3]):  # Show first 3 questions
                logger.info(f"   Q{i+1}: {q.get('text', '')[:100]}...")
        else:
            logger.info("📄 Using pre-extracted questions...")
            logger.info(f"✅ Using {len(questions)} pre-extracted questions")
        
        # Step 2: Validate and prepare UoC data
        logger.info("🔧 Step 2: Preparing UoC data...")
        prepared_uoc_data = prepare_uoc_data(uoc_data)
        
        # Step 3: Map questions to UoC components
        logger.info("🗺️  Step 3: Mapping questions to UoC components...")
        from agents.mapping_agent import MappingAgent
        
        # Use testing_mode=True to catch errors early
        mapping_agent = MappingAgent(
            api_key=api_key,
            testing_mode=True
        )
        
        # Determine assessment type
        assessment_type = determine_assessment_type(questions)
        logger.info(f"📋 Assessment type determined: {assessment_type}")
        
        # Perform mapping
        mapping_results = mapping_agent.execute(
            questions=questions,
            uoc_data=prepared_uoc_data,
            assessment_type=assessment_type
        )
        
        if not mapping_results or not mapping_results.get('mappings'):
            logger.error("No mappings generated by LLM. Aborting per no-fallback policy.")
            raise RuntimeError('LLM mapping failed: no mappings generated')
        
        logger.info(f"✅ Generated {len(mapping_results.get('mappings', []))} mappings")
        
        # Step 4: Compile final results
        results = {
            'success': True,
            'questions': questions,
            'mappings': mapping_results.get('mappings', []),  # Extract the mappings list
            'statistics': {
                'total_questions': len(questions),
                'total_mappings': len(mapping_results.get('mappings', [])),
                'assessment_type': assessment_type,
                'uoc_info': {
                    'code': prepared_uoc_data.get('uoc_code', 'Unknown'),
                    'title': prepared_uoc_data.get('title', 'Unknown')
                }
            }
        }
        
        logger.info("🎉 Assessment analysis completed successfully!")
        return results
        
    except Exception as e:
        logger.error(f"❌ Error in assessment analysis: {str(e)}")
        raise

def initialize_collaborative_validation(session_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Initialize collaborative validation for a completed mapping session
    
    Args:
        session_data: Session data with completed mappings
        
    Returns:
        Dict: Session data enhanced with validation threads
    """
    try:
        logger.info("🔄 Initializing collaborative validation...")
        
        from agents.validation_agent import CollaborativeValidationAgent
        
        validation_agent = CollaborativeValidationAgent()
        
        # Initialize validation threads
        validation_summary = validation_agent.execute(session_data)
        
        # Add validation summary to session data
        session_data['validation_summary'] = validation_summary
        session_data['validation_initialized'] = True
        session_data['validation_initialized_at'] = validation_agent._extract_system_rationales({}).get('timestamp', '')
        
        logger.info(f"✅ Collaborative validation initialized for {validation_summary.get('total_mappings', 0)} mappings")
        
        return session_data
        
    except Exception as e:
        logger.error(f"❌ Error initializing collaborative validation: {str(e)}")
        session_data['validation_error'] = str(e)
        return session_data

def prepare_uoc_data(uoc_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Prepare and validate UoC data structure
    
    Args:
        uoc_data (Dict): Raw UoC data
        
    Returns:
        Dict: Prepared UoC data with consistent structure
    """
    try:
        # Handle different UoC data formats
        if 'data' in uoc_data:
            # Original format from cache
            original_data = uoc_data['data']
            prepared_data = {
                'uoc_code': uoc_data.get('uoc_code', 'Unknown'),
                'title': uoc_data.get('title', 'Unknown'),
                'elements': original_data.get('elements', []),
                'performance_criteria': original_data.get('performance_criteria', []),
                'performance_evidence': original_data.get('performance_evidence', []),
                'knowledge_evidence': original_data.get('knowledge_evidence', [])
            }
        else:
            # Direct format
            prepared_data = {
                'uoc_code': uoc_data.get('uoc_code', 'Unknown'),
                'title': uoc_data.get('title', 'Unknown'),
                'elements': uoc_data.get('elements', []),
                'performance_criteria': uoc_data.get('performance_criteria', []),
                'performance_evidence': uoc_data.get('performance_evidence', []),
                'knowledge_evidence': uoc_data.get('knowledge_evidence', [])
            }
        
        # Convert dict formats to lists if needed
        for key in ['elements', 'performance_criteria', 'performance_evidence', 'knowledge_evidence']:
            if isinstance(prepared_data[key], dict):
                prepared_data[key] = list(prepared_data[key].values())
        
        # Validate that we have some UoC components
        total_components = (
            len(prepared_data['elements']) +
            len(prepared_data['performance_criteria']) +
            len(prepared_data['performance_evidence']) +
            len(prepared_data['knowledge_evidence'])
        )
        
        if total_components == 0:
            logger.warning("⚠️  UoC data appears to be empty!")
        else:
            logger.info(f"📊 UoC components: {len(prepared_data['elements'])} elements, "
                       f"{len(prepared_data['performance_criteria'])} criteria, "
                       f"{len(prepared_data['performance_evidence'])} PE, "
                       f"{len(prepared_data['knowledge_evidence'])} KE")
        
        return prepared_data
        
    except Exception as e:
        logger.error(f"Error preparing UoC data: {str(e)}")
        # Return minimal structure
        return {
            'uoc_code': 'Unknown',
            'title': 'Unknown',
            'elements': [],
            'performance_criteria': [],
            'performance_evidence': [],
            'knowledge_evidence': []
        }

def determine_assessment_type(questions: List[Dict[str, Any]]) -> str:
    """
    Determine the overall assessment type based on question types
    
    Args:
        questions (List[Dict]): List of extracted questions
        
    Returns:
        str: Assessment type (KBA, SBA, PEP, Mixed)
    """
    try:
        if not questions:
            return "Mixed"
        
        question_types = [q.get('question_type', 'unknown') for q in questions]
        
        # Count different types
        knowledge_types = ['mcq', 'short_answer', 'true_false']
        skill_types = ['practical', 'demonstration', 'observation']
        portfolio_types = ['portfolio', 'evidence_collection']
        
        knowledge_count = sum(1 for qt in question_types if qt in knowledge_types)
        skill_count = sum(1 for qt in question_types if qt in skill_types)
        portfolio_count = sum(1 for qt in question_types if qt in portfolio_types)
        
        total = len(questions)
        
        # Determine primary type
        if knowledge_count / total > 0.7:
            return "KBA"
        elif skill_count / total > 0.7:
            return "SBA"
        elif portfolio_count / total > 0.7:
            return "PEP"
        else:
            return "Mixed"
            
    except Exception as e:
        logger.error(f"Error determining assessment type: {str(e)}")
        return "Mixed"

# Example usage
if __name__ == "__main__":
    # Example 1: From file
    file_path = "path/to/assessment.docx"
    uoc_data = {
        'uoc_code': 'CHCAGE011',
        'title': 'Provide support to people living with dementia',
        'elements': [
            {'id': 'E1', 'description': 'Establish person centred approaches to support'},
            {'id': 'E2', 'description': 'Facilitate a safe and supportive environment'}
        ],
        'performance_criteria': [
            {'code': 'PC1.1', 'description': 'Support needs and preferences are identified'},
            {'code': 'PC1.2', 'description': 'Person centred approach is implemented'}
        ],
        'performance_evidence': [],
        'knowledge_evidence': []
    }
    
    # Run analysis
    results = run_assessment_analysis(
        file_path=file_path,
        uoc_data=uoc_data,
        api_key="your_api_key_here"  # Optional - will use mock data if not provided
    )
    
    if results['success']:
        print(f"✅ Analysis completed: {results['statistics']['total_questions']} questions, "
              f"{results['statistics']['total_mappings']} mappings")
    else:
        print(f"❌ Analysis failed: {results['error']}")

    # Example 2: From text content
    text_content = """
    4. Person centered approach is the best way to care for a person with dementia. 
    What personal understanding and approach would you use to deliver a person centered care? 
    Select four (4) answers.
    
    A. Putting each client at the centre of care
    B. Providing clients with opportunities that support self esteem  
    C. Empowering clients
    D. Adapting care to suit each client
    """
    
    results = run_assessment_analysis(
        text_content=text_content,
        uoc_data=uoc_data,
        api_key="your_api_key_here"
    )

    if results['success']:
        print(f"✅ Text analysis completed: {results['statistics']['total_questions']} questions")
    else:
        print(f"❌ Text analysis failed: {results['error']}") 